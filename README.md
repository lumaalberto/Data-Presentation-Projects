# Data-Presentation-Projects

I started this project on Tuesday night. I just completed it this Saturday afternoon. I was getting so many errors throughout the project that I was about to give up. It is extremely stressful when I understand something but I am not getting the result that I expect. Then, I decided to use another approach which worked for me perfectly. I decided to stop working on the project and read the book so I could have a better understanding of what my result should be. Throughout the project, there has been one major question that I kept in mind because I wanted to focus on a great result. What have I learned so far in during this course to make my project easier to complete?

Well, I had learned that Data Wrangling is the process of transforming or cleansing data that flows from a source to a target. Just like when cleaning data, the most important things is to know what size of data that I am dealing with. Data wrangling is actually the same process. For small datasets that can be opened in excel, the transformations of cleansing rules can be defined in excel through the help of macros. When the dataset size is such that it cannot be opened in excel you can then use scripting or programming languages to do the same operation and schedule them using the schedulers mentioned example. Such scripting languages are typically running on a single machine and the performance is directly proportional to the configuration of the machine. When the dataset is large, opening in excel is not possible and running the cleansing rule on a single machine can be slow. This is where Big Data technologies such as Map reduce and Spark shine where a subset of data is sent to multiple machines and the cleansing rule is applied on each machine on the subset of data thereby increasing the throughput of the entire processing.
